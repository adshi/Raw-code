{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group member\n",
    " - Sneha Agarwal\n",
    " - Nalini Agrawal\n",
    " - Qijing Zhang (Vicky)\n",
    " - Wenyue Shi (Ada)\n",
    " - Yuwen Wang\n",
    " - Suraj Tata Prasad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A. \n",
    "### Ignore the text (reviews) and run a classification model with the numeric data (you can use standard methods like logistic regression, k-nearest neighbors or anything else). What is the best accuracy of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy.random as npr\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the target column for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['votes_cool', 'votes_funny', 'votes_useful', 'Cheap', 'Moderate',\n",
       "       'Expensive', 'VeryExpensive', 'American', 'Chinese', 'French',\n",
       "       'Japanese', 'Indian', 'Italian', 'Greek', 'Mediterranean',\n",
       "       'Mexican', 'Thai', 'Vietnamese', 'Others', 'target'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n92kDpCwX0vsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10954ba10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv('yelp.csv')\n",
    "# drop the review column\n",
    "df = df.drop(['Review'], axis = 1)\n",
    "# create a dummy variable for stars\n",
    "df['target'] = df['stars'] > 3\n",
    "df['target'].hist()\n",
    "df = df.drop(['stars'], axis = 1)\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create X and y values and split between train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formula = 'target ~ 0 + votes_cool + votes_funny + votes_useful + Cheap + Moderate + Expensive + VeryExpensive + \\\n",
    "           American + Chinese + French + Japanese + Indian + Italian + Greek + Mediterranean + Mexican + Thai + \\\n",
    "           Vietnamese + Others'\n",
    "Y, X = dmatrices(formula, df, return_type='dataframe') \n",
    "y = Y['target[True]']\n",
    "# split the train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.3, random_state=1)\n",
    "\n",
    "# resample from the low rates because we have fewer low rates records\n",
    "n = len(y[y==1])\n",
    "index_high = [i for i in y.index if y[i]]\n",
    "index_low = [i for i in y.index if not y[i]]\n",
    "new_low = npr.choice(index_low, size = n, replace = True)\n",
    "high_X = X.ix[index_high]\n",
    "high_y = y[index_high]\n",
    "low_X = X.ix[new_low]\n",
    "low_y = y[new_low]\n",
    "new_X = np.concatenate((low_X, high_X), axis=0)\n",
    "new_y = np.concatenate((low_y, high_y), axis = 0)\n",
    "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(new_X, new_y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "model_list = [('Naive Bayes', naive_bayes.MultinomialNB()), \n",
    "              ('K-nn', neighbors.KNeighborsClassifier(n_neighbors = 10, weights = 'uniform', p = 2)),\n",
    "              ('logistic regression', LogisticRegression()),\n",
    "              ('Decision Tree', tree.DecisionTreeClassifier(criterion='entropy')),\n",
    "              ('Bagging', BaggingClassifier(tree.DecisionTreeClassifier(criterion='entropy'), random_state=1)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=10, random_state=1)), \n",
    "              ('Gradient Boosting', GradientBoostingClassifier(n_estimators=200, max_depth=4, random_state=1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Naive Bayes\n",
      "Fitting K-nn\n",
      "Fitting logistic regression\n",
      "Fitting Decision Tree\n",
      "Fitting Bagging\n",
      "Fitting Random Forest\n",
      "Fitting Gradient Boosting\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>0.667667</td>\n",
       "      <td>0.731195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.734195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.691333</td>\n",
       "      <td>0.703336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K-nn</th>\n",
       "      <td>0.614167</td>\n",
       "      <td>0.656547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.679500</td>\n",
       "      <td>0.677620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.670333</td>\n",
       "      <td>0.731124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic regression</th>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.684977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Test Accuracy  Training Accuracy\n",
       "Bagging                   0.667667           0.731195\n",
       "Decision Tree             0.665000           0.734195\n",
       "Gradient Boosting         0.691333           0.703336\n",
       "K-nn                      0.614167           0.656547\n",
       "Naive Bayes               0.679500           0.677620\n",
       "Random Forest             0.670333           0.731124\n",
       "logistic regression       0.685000           0.684977"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for (name, model) in model_list: \n",
    "    print 'Fitting', name\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "accuracy_train = {} \n",
    "accuracy_test = {}\n",
    "for (name, model) in model_list:\n",
    "    prediction_train = model.predict(X_train)\n",
    "    accuracy_train[name] = metrics.accuracy_score(y_train, prediction_train) \n",
    "    prediction_test = model.predict(X_test)\n",
    "    accuracy_test[name] = metrics.accuracy_score(y_test, prediction_test)\n",
    "\n",
    "df = pd.DataFrame({'Training Accuracy':accuracy_train, 'Test Accuracy':accuracy_test}) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The highest accuracy is about 69%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B\n",
    "### Perform a supervised classification on a subset of the corpus using the reviews only. You can write your code in Python or R. What accuracy do you get from this text mining exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "df = pd.read_csv('yelp.csv')\n",
    "review = [r.decode('ascii', 'ignore') for r in df['Review']]\n",
    "y = df['stars'] > 3\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(review)\n",
    "X = X.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.3, random_state=1)\n",
    "\n",
    "n = len(y[y==1])\n",
    "index_high = [i for i in y.index if y[i]]\n",
    "index_low = [i for i in y.index if not y[i]]\n",
    "new_low = npr.choice(index_low, size = n, replace = True)\n",
    "high_X = X[index_high]\n",
    "high_y = y[index_high]\n",
    "low_X = X[new_low]\n",
    "low_y = y[new_low]\n",
    "new_X = np.concatenate((low_X, high_X), axis=0)\n",
    "new_y = np.concatenate((low_y, high_y), axis = 0)\n",
    "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(new_X, new_y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Naive Bayes\n",
      "Fitting logistic regression\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.843500</td>\n",
       "      <td>0.899636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic regression</th>\n",
       "      <td>0.844167</td>\n",
       "      <td>0.980713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Test Accuracy  Training Accuracy\n",
       "Naive Bayes               0.843500           0.899636\n",
       "logistic regression       0.844167           0.980713"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list = [('Naive Bayes', naive_bayes.MultinomialNB()), \n",
    "              ('logistic regression', LogisticRegression())]\n",
    "\n",
    "for (name, model) in model_list: \n",
    "    print 'Fitting', name\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "accuracy_train = {} \n",
    "accuracy_test = {}\n",
    "for (name, model) in model_list:\n",
    "    prediction_train = model.predict(X_train)\n",
    "    accuracy_train[name] = metrics.accuracy_score(y_train, prediction_train) \n",
    "    prediction_test = model.predict(X_test)\n",
    "    accuracy_test[name] = metrics.accuracy_score(y_test, prediction_test)\n",
    "\n",
    "df = pd.DataFrame({'Training Accuracy':accuracy_train, 'Test Accuracy':accuracy_test}) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting all models after resampling\n",
      "Fitting Naive Bayes\n",
      "Fitting logistic regression\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.847639</td>\n",
       "      <td>0.894066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic regression</th>\n",
       "      <td>0.893753</td>\n",
       "      <td>0.983556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Test Accuracy  Training Accuracy\n",
       "Naive Bayes               0.847639           0.894066\n",
       "logistic regression       0.893753           0.983556"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Fitting all models after resampling\"\n",
    "for (name, model) in model_list: \n",
    "    print 'Fitting', name\n",
    "    model.fit(X_new_train, y_new_train)\n",
    "    \n",
    "accuracy_train = {} \n",
    "accuracy_test = {}\n",
    "for (name, model) in model_list:\n",
    "    prediction_train = model.predict(X_new_train)\n",
    "    accuracy_train[name] = metrics.accuracy_score(y_new_train, prediction_train) \n",
    "    prediction_test = model.predict(X_new_test)\n",
    "    accuracy_test[name] = metrics.accuracy_score(y_new_test, prediction_test)\n",
    "\n",
    "df = pd.DataFrame({'Training Accuracy':accuracy_train, 'Test Accuracy':accuracy_test}) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The highest accuracy is 89.4%, which comes from logistic regression after resampling from low ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task C\n",
    "### Combine the numeric data and the text classification model (in task B) to create a “hybrid” model. It is your task to figure out how to do this. Now run this hybrid classification model and compare the results with those in A and B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('yelp.csv')\n",
    "review = [r.decode('ascii', 'ignore') for r in df['Review']]\n",
    "others = df.drop(['Review', 'stars'], axis = 1)\n",
    "y = df['stars'] > 3\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "X = vectorizer.fit_transform(review)\n",
    "main_X = X.toarray()\n",
    "other_X = others.as_matrix()\n",
    "X = np.concatenate((main_X, other_X), axis=1)\n",
    "X.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.3, random_state=1)\n",
    "\n",
    "n = len(y[y==1])\n",
    "index_high = [i for i in y.index if y[i]]\n",
    "index_low = [i for i in y.index if not y[i]]\n",
    "new_low = npr.choice(index_low, size = n, replace = True)\n",
    "high_X = X[index_high]\n",
    "high_y = y[index_high]\n",
    "low_X = X[new_low]\n",
    "low_y = y[new_low]\n",
    "new_X = np.concatenate((low_X, high_X), axis=0)\n",
    "new_y = np.concatenate((low_y, high_y), axis = 0)\n",
    "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(new_X, new_y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Naive Bayes\n",
      "Fitting logistic regression\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.836167</td>\n",
       "      <td>0.89885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic regression</th>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.97607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Test Accuracy  Training Accuracy\n",
       "Naive Bayes               0.836167            0.89885\n",
       "logistic regression       0.837500            0.97607"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list = [('Naive Bayes', naive_bayes.MultinomialNB()), \n",
    "              ('logistic regression', LogisticRegression())]\n",
    "\n",
    "for (name, model) in model_list: \n",
    "    print 'Fitting', name\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "accuracy_train = {} \n",
    "accuracy_test = {}\n",
    "for (name, model) in model_list:\n",
    "    prediction_train = model.predict(X_train)\n",
    "    accuracy_train[name] = metrics.accuracy_score(y_train, prediction_train) \n",
    "    prediction_test = model.predict(X_test)\n",
    "    accuracy_test[name] = metrics.accuracy_score(y_test, prediction_test)\n",
    "\n",
    "df = pd.DataFrame({'Training Accuracy':accuracy_train, 'Test Accuracy':accuracy_test}) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting all models after resampling\n",
      "Fitting Naive Bayes\n",
      "Fitting logistic regression\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.850590</td>\n",
       "      <td>0.897439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic regression</th>\n",
       "      <td>0.886375</td>\n",
       "      <td>0.980816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Test Accuracy  Training Accuracy\n",
       "Naive Bayes               0.850590           0.897439\n",
       "logistic regression       0.886375           0.980816"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Fitting all models after resampling\"\n",
    "for (name, model) in model_list: \n",
    "    print 'Fitting', name\n",
    "    model.fit(X_new_train, y_new_train)\n",
    "    \n",
    "accuracy_train = {} \n",
    "accuracy_test = {}\n",
    "for (name, model) in model_list:\n",
    "    prediction_train = model.predict(X_new_train)\n",
    "    accuracy_train[name] = metrics.accuracy_score(y_new_train, prediction_train) \n",
    "    prediction_test = model.predict(X_new_test)\n",
    "    accuracy_test[name] = metrics.accuracy_score(y_new_test, prediction_test)\n",
    "\n",
    "df = pd.DataFrame({'Training Accuracy':accuracy_train, 'Test Accuracy':accuracy_test}) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most informative attributes for high ratings\n",
      "votes_useful   -3.838853\n",
      "votes_cool     -4.217330\n",
      "food           -4.443807\n",
      "good           -4.469989\n",
      "place          -4.568856\n",
      "votes_funny    -4.625387\n",
      "great          -4.653426\n",
      "Moderate       -4.685982\n",
      "like           -5.023096\n",
      "just           -5.119353\n",
      "dtype: float64\n",
      "most infromative attributes for low ratings\n",
      "possable                 -13.37258\n",
      "dubiousness              -13.37258\n",
      "dubbya                   -13.37258\n",
      "duality                  -13.37258\n",
      "possessive               -13.37258\n",
      "dtpav4wfappezx_pvczrxa   -13.37258\n",
      "possiblity               -13.37258\n",
      "postage                  -13.37258\n",
      "drysince                 -13.37258\n",
      "wkend                    -13.37258\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "index = vectorizer.get_feature_names()\n",
    "other_name = others.columns.values\n",
    "all = list(index) + list(other_name)\n",
    "\n",
    "for (name, model) in model_list:\n",
    "    if name == \"Naive Bayes\":\n",
    "        most_info= pd.Series(model.coef_[0], index = all)\n",
    "        most_info.sort(ascending = False)\n",
    "        print \"most informative attributes for high ratings\"\n",
    "        print most_info[:10]\n",
    "        print \"most infromative attributes for low ratings\"\n",
    "        print most_info[-10:]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy in part C is much higher than part A but slightly lower than part B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task D\n",
    "### Use unsupervised sentiment analysis on the reviews (with SentiStrength or any other tool) and use the sentiment score to predict high/low rating. Compare and contrast the results of tasks B and D. What can you conclude from your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression\n",
      "Using the sum of Positive and Negative score as X\n",
      "The accuracy is 0.724625\n"
     ]
    }
   ],
   "source": [
    "reviews_rated = pd.read_csv('reviews1_out.txt',sep='\\t')\n",
    "X = reviews_rated['Positive'] + reviews_rated['Negative']\n",
    "X = X.drop(0, axis=0)\n",
    "X = np.reshape(X, newshape=(len(X), 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.4, random_state=1)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Fitting Logistic Regression\"\n",
    "print \"Using the sum of Positive and Negative score as X\"\n",
    "print \"The accuracy is\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression\n",
      "Using both Positive and Negative score as X\n",
      "The accuracy is 0.736125\n"
     ]
    }
   ],
   "source": [
    "reviews_rated = pd.read_csv('reviews1_out.txt', sep = '\\t')\n",
    "X = reviews_rated[['Positive', 'Negative']]\n",
    "X = X.drop(0, axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.4, random_state=1)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Fitting Logistic Regression\"\n",
    "print \"Using both Positive and Negative score as X\"\n",
    "print \"The accuracy is\", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy in part D is much lower than that of part B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task E\n",
    "### Use unsupervised clustering on the text. Does clustering achieve “good” separation between high and low rated restaurants? How can you explain the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "df = pd.read_csv('yelp.csv')\n",
    "review = [r.decode('ascii', 'ignore') for r in df['Review']]\n",
    "y = df['stars'] > 4\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(review)\n",
    "\n",
    "# Dimention reduction, similar to PCA, choose the first 5 components\n",
    "svd = TruncatedSVD(n_components = 5, random_state = 2)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "X = lsa.fit_transform(X)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.3)\n",
    "\n",
    "n = len(y[y==1])\n",
    "index_high = [i for i in y.index if y[i]]\n",
    "index_low = [i for i in y.index if not y[i]]\n",
    "new_low = npr.choice(index_low, size = n, replace = True)\n",
    "high_X = X[index_high]\n",
    "high_y = y[index_high]\n",
    "low_X = X[new_low]\n",
    "low_y = y[new_low]\n",
    "new_X = np.concatenate((low_X, high_X), axis=0)\n",
    "new_y = np.concatenate((low_y, high_y), axis = 0)\n",
    "# X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(new_X, new_y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None [[ 0.88882547 -0.43928777  0.07952773  0.08716016 -0.05562368]\n",
      " [ 0.59046521  0.39408481 -0.19534897  0.09015685  0.67064038]\n",
      " [ 0.80498933 -0.02921724  0.09920451 -0.14959925  0.56472742]\n",
      " ..., \n",
      " [ 0.72354625  0.6568264   0.12283658 -0.17178259  0.02149005]\n",
      " [ 0.9458513  -0.21672512  0.0196899  -0.12284596 -0.20716351]\n",
      " [ 0.63505422  0.17858968  0.23744712 -0.33878908 -0.62741748]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.cluster import KMeansClusterer, cosine_distance\n",
    "clusterer = KMeansClusterer(2, cosine_distance, repeats=100)\n",
    "clusters = clusterer.cluster(X, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61688084404220211"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print('Clustered:', vectors)\n",
    "metrics.accuracy_score(y, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: means will be discarded for subsequent trials\n",
      "None [[ 0.92624309  0.10728293  0.21678     0.02418972 -0.28807186]\n",
      " [ 0.82954455  0.05633541  0.12262057 -0.14458779  0.52224585]\n",
      " [ 0.96668246 -0.23292961 -0.04726275  0.09422988 -0.01248148]\n",
      " ..., \n",
      " [ 0.82214436  0.29873822  0.21410073 -0.33081528 -0.28205718]\n",
      " [ 0.72354625  0.6568264   0.12283658 -0.17178259  0.02149005]\n",
      " [ 0.63505422  0.17858968  0.23744712 -0.33878908 -0.62741748]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.59865215979214026"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_resample = clusterer.cluster(new_X, True)\n",
    "metrics.accuracy_score(new_y, clusters_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60983049152457625"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(init = 'k-means++', n_clusters=2, random_state=1, max_iter=100, n_init=10)\n",
    "kmeans.fit(X)\n",
    "# scatter(X[:,0], X[:,1], c=y.values, cmap = 'gist_ncar')\n",
    "# scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap='gist_ncar')\n",
    "# scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], marker=\"x\", s=200, linewidths=5)\n",
    "prediction = kmeans.labels_\n",
    "metrics.accuracy_score(y, prediction)\n",
    "# metrics.adjusted_rand_score(y, prediction) \n",
    "# print(\"Top terms per cluster:\")\n",
    "# order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "# terms = vectorizer.get_feature_names()\n",
    "# for i in range(2):\n",
    "#     print(\"Cluster %d:\" % i)\n",
    "#     for ind in order_centroids[i, :10]:\n",
    "#         print(' %s' % terms[ind])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In part E, the unsupervised clustering gave pretty low accuracy compared with previous parts. \n",
    "After assigning 5 star to be high rating and 1-4 star to be low rating, the accuracy increased from 40% to 60%, which is still the lowest of the six parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
